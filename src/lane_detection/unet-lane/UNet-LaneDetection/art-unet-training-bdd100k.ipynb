{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.224626Z","iopub.status.busy":"2022-10-15T04:41:42.224278Z","iopub.status.idle":"2022-10-15T04:41:42.229058Z","shell.execute_reply":"2022-10-15T04:41:42.227898Z","shell.execute_reply.started":"2022-10-15T04:41:42.224586Z"},"id":"MvWDYl2mhXSw","trusted":true},"outputs":[],"source":["# Import Modules"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"96y2Pkb3hZMK","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","import cv2\n","import numpy as np\n","import os\n","import PIL\n","import matplotlib.pyplot as plt\n","import random\n","import math\n","import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"SK6ZjGZmjWmN"},"source":["# Defining the UNet model and the custom dataset class"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.249636Z","iopub.status.busy":"2022-10-15T04:41:42.249378Z","iopub.status.idle":"2022-10-15T04:41:42.277688Z","shell.execute_reply":"2022-10-15T04:41:42.276677Z","shell.execute_reply.started":"2022-10-15T04:41:42.249607Z"},"id":"0dWr7cYhjT_o","trusted":true},"outputs":[],"source":["class LaneDataset(Dataset):\n","    def __init__(self,imagePath,maskPath,prob=0,transforms=None):\n","        self.imagePath = imagePath # Array of filepaths for the input images\n","        self.maskPath = maskPath # Array of filepaths for the mask images\n","        self.transforms = transforms\n","        self.prob = prob\n","\n","    def __len__(self):\n","        return len(self.imagePath)\n","\n","    def __getitem__(self,idx):\n","        img_path = self.imagePath[idx]\n","        mask_path = self.maskPath[idx]\n","\n","        image = cv2.imread(img_path)\n","        if image is None:\n","            print(img_path)\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","        mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n","\n","        edges,edges_inv = self.find_edge_channel(image)\n","        \n","        gradient_map = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=-1) # Gradient map along x\n","#         gradient_map = cv2.Laplacian(gray, cv2.CV_64F)\n","        gradient_map = np.uint8(np.absolute(gradient_map))\n","        \n","        output_image = np.zeros((gray.shape[0],gray.shape[1],3),dtype=np.uint8)\n","        output_image[:,:,0] = gray\n","        output_image[:,:,1] = edges\n","        output_image[:,:,2] = gradient_map\n","        \n","        if \"Town\" in mask_path:\n","            output_image, mask = self.prob_rotate(output_image,mask)\n","            output_image, mask = self.prob_flip(output_image,mask)\n","#             mask = cv2.bitwise_or(cv2.bitwise_and(mask,edges),cv2.bitwise_and(mask,edges_inv))\n","\n","        if self.transforms != None:\n","            output_image = self.transforms(output_image)\n","#             output_image = self.transforms(image)\n","#             edges = self.transforms(edges)\n","#             edges_inv = self.transforms(edges_inv)\n","            mask = self.transforms(mask)\n","                \n","#         output_image = torch.cat((output_image,edges),dim=0)\n","#         output_image = torch.cat((output_image,edges_inv),dim=0)\n","        \n","        mask_binary = (mask>0).type(torch.float)\n","        \n","        return (output_image,mask_binary, img_path)\n","    \n","    def prob_flip(self,img,lbl):\n","        if random.random() > self.prob:\n","            return img,lbl\n","        flip_img = cv2.flip(img,1)\n","        flip_lbl = cv2.flip(lbl,1)\n","        return flip_img,flip_lbl\n","    \n","    def prob_rotate(self,img,lbl):\n","        if random.random() > self.prob: \n","            return img,lbl\n","        \n","        rotations = [-90,-45,45,90,180]\n","        angle = random.choice(rotations)\n","        center_img = (img.shape[1]//2,img.shape[0]//2)\n","        center_lbl = (lbl.shape[1]//2,lbl.shape[0]//2)\n","        \n","        rotate_matrix_img = cv2.getRotationMatrix2D(center=center_img,angle=angle,scale=1)\n","        rotate_matrix_lbl = cv2.getRotationMatrix2D(center=center_lbl,angle=angle,scale=1)\n","        \n","        rotated_img = cv2.warpAffine(img,rotate_matrix_img,(img.shape[1],img.shape[0]))\n","        rotated_lbl = cv2.warpAffine(lbl,rotate_matrix_lbl,(lbl.shape[1],lbl.shape[0]))\n","        \n","        return rotated_img,rotated_lbl\n","    \n","    def find_edge_channel(self,img):\n","        edges_mask = np.zeros((img.shape[0],img.shape[1]),dtype=np.uint8)\n","        width = img.shape[1]\n","        height = img.shape[0]\n","        \n","        gray_im = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n","        \n","        # Separate into quadrants\n","        med1 = np.median(gray_im[:height//2,:width//2])\n","        med2 = np.median(gray_im[:height//2,width//2:])\n","        med3 = np.median(gray_im[height//2:,width//2:])\n","        med4 = np.median(gray_im[height//2:,:width//2])\n","\n","        l1 = int(max(0,(1-0.205)*med1))\n","        u1 = int(min(255,(1+0.205)*med1))\n","        e1 = cv2.Canny(gray_im[:height//2,:width//2],l1,u1)\n","\n","        l2 = int(max(0,(1-0.205)*med2))\n","        u2 = int(min(255,(1+0.205)*med2))\n","        e2 = cv2.Canny(gray_im[:height//2,width//2:],l2,u2)\n","\n","        l3 = int(max(0,(1-0.205)*med3))\n","        u3 = int(min(255,(1+0.205)*med3))\n","        e3 = cv2.Canny(gray_im[height//2:,width//2:],l3,u3)\n","\n","        l4 = int(max(0,(1-0.205)*med4))\n","        u4 = int(min(255,(1+0.205)*med4))\n","        e4 = cv2.Canny(gray_im[height//2:,:width//2],l4,u4)\n","\n","        # Stitch the edges together\n","        edges_mask[:height//2,:width//2] = e1\n","        edges_mask[:height//2,width//2:] = e2\n","        edges_mask[height//2:,width//2:] = e3\n","        edges_mask[height//2:,:width//2] = e4\n","        \n","        edges_mask_inv = cv2.bitwise_not(edges_mask)\n","        \n","        return edges_mask, edges_mask_inv"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.279572Z","iopub.status.busy":"2022-10-15T04:41:42.279172Z","iopub.status.idle":"2022-10-15T04:41:42.296836Z","shell.execute_reply":"2022-10-15T04:41:42.295983Z","shell.execute_reply.started":"2022-10-15T04:41:42.279531Z"},"id":"S7TwobN4pP9y","trusted":true},"outputs":[],"source":["from operator import indexOf\n","from numpy import ndim\n","import torch.nn as nn\n","import torch\n","\n","\n","class ConvStage(nn.Module):\n","    def __init__(self, in_channels, out_channels) -> None:\n","        super(ConvStage, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=1, features=[16, 32, 64, 128]):\n","        super(UNet, self).__init__()\n","\n","        self.encoder = nn.ModuleList()\n","        self.decoder = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # Setting up the architecture for the encoder\n","        for feature in features:\n","            double_conv = ConvStage(\n","                in_channels=in_channels, out_channels=feature)\n","            self.encoder.append(double_conv)\n","            in_channels = feature\n","\n","        self.bottleneck = ConvStage(\n","            in_channels=features[-1], out_channels=features[-1] * 2\n","        )\n","\n","        # Setting up the architecrue for the decoder\n","        for feature in reversed(features):\n","            up_conv = nn.ConvTranspose2d(\n","                in_channels=feature * 2, out_channels=feature, kernel_size=2, stride=2\n","            )\n","            self.decoder.append(up_conv)\n","            double_conv = ConvStage(\n","                in_channels=feature * 2, out_channels=feature)\n","            self.decoder.append(double_conv)\n","\n","        self.segmentation = nn.Conv2d(\n","            in_channels=features[0], out_channels=out_channels, kernel_size=1, stride=1\n","        )\n","\n","    def forward(self, x):\n","        # Make sure that the inputted size is compatible\n","        assert x.shape[2] % 16 == 0 and x.shape[3] % 16 == 0\n","\n","        copies = []\n","\n","        # Forward pass through the encoder\n","        for i, down in enumerate(self.encoder):\n","            x = down(x)\n","            # Store a copy\n","            copies.append(x)\n","            x = self.pool(x)\n","\n","        # The bottleneck\n","        x = self.bottleneck(x)\n","\n","        # Reverse the coppies\n","        copies = copies[::-1]\n","\n","        # Forward pass through the decoder\n","        for j, up in enumerate(self.decoder):\n","            if j % 2 == 0:\n","                x = up(x)\n","                x = torch.cat((copies[j // 2], x), axis=1)\n","            else:\n","                x = up(x)\n","\n","        return self.segmentation(x)\n","\n","\n","def test():\n","    x = torch.rand((3, 2, 160, 256))\n","    model = UNet(in_channels=2, out_channels=1)\n","    pred = model(x)\n","    print(x.shape, pred.shape)\n","    \n","# test()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.382018Z","iopub.status.busy":"2022-10-15T04:41:42.381524Z","iopub.status.idle":"2022-10-15T04:41:42.387790Z","shell.execute_reply":"2022-10-15T04:41:42.386743Z","shell.execute_reply.started":"2022-10-15T04:41:42.381978Z"},"trusted":true},"outputs":[],"source":["# def dice_loss(pred, target, smooth = 1.):\n","#     pred = pred.contiguous()\n","#     target = target.contiguous()    \n","\n","#     intersection = (pred * target).sum(dim=2).sum(dim=2)\n","    \n","#     loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n","    \n","#     return loss.mean()\n","\n","def dice_loss(inputs, target):\n","    num = target.size(0)\n","    inputs = inputs.reshape(num, -1)\n","    target = target.reshape(num, -1)\n","    smooth = 1.0\n","    intersection = (inputs * target)\n","    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n","    dice = 1 - dice.sum() / num\n","    return dice"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.389921Z","iopub.status.busy":"2022-10-15T04:41:42.389505Z","iopub.status.idle":"2022-10-15T04:41:42.399963Z","shell.execute_reply":"2022-10-15T04:41:42.399023Z","shell.execute_reply.started":"2022-10-15T04:41:42.389876Z"},"trusted":true},"outputs":[],"source":["def calc_loss(pred, target, metrics, bce_weight=0.5):\n","    bce = F.binary_cross_entropy_with_logits(pred, target)\n","        \n","    pred = torch.sigmoid(pred)\n","    dice = dice_loss(pred, target)\n","    \n","    loss = bce * bce_weight + dice * (1 - bce_weight)\n","    \n","    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n","    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n","    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n","    \n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"6dRaDZv9pfKP"},"source":["# Training Code and Loop"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-10-15T04:41:42.407080Z","iopub.status.busy":"2022-10-15T04:41:42.406314Z","iopub.status.idle":"2022-10-15T04:41:42.670628Z","shell.execute_reply":"2022-10-15T04:41:42.669365Z","shell.execute_reply.started":"2022-10-15T04:41:42.407033Z"},"id":"Kq_KRUSZpjXQ","outputId":"81cb6c82-9f51-4209-9b2f-daffba0b003f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5312 5312\n","4249 train images,4249 train labels\n","1063 val images,1063 val labels\n"]}],"source":["torch.manual_seed(250)\n","random.seed(2)\n","\n","def sortKey(x):\n","    x1 = x.split(\"/\")[-1]\n","    x2 = x1.split(\"_\")[-1]\n","    val = int(x2.split(\".\")[0])\n","    return val\n","\n","def accuracy(model,dataloader):\n","    if torch.cuda.is_available():\n","        model.cuda()\n","\n","    model.eval()\n","    total_correct = 0\n","    total_inputs = 0\n","    for i,data in enumerate(dataloader,0):\n","        image,label = data\n","\n","        if torch.cuda.is_available():\n","            image = image.cuda()\n","            label = label.cuda()\n","\n","        out = model(image)\n","        pred = torch.sigmoid(out)\n","        pred_np = pred.detach().cpu().numpy()\n","        labels_np = label.detach().cpu().numpy()\n","\n","        masked_pred = np.where(pred_np>0.017,1,0) # Thresholds prediction\n","        correct = (masked_pred==labels_np.astype('int64')).sum()\n","        incorrect = (masked_pred!=labels_np.astype('int64')).sum()\n","\n","        total_correct += correct\n","        total_inputs += correct+incorrect\n","\n","    if total_inputs == 0:\n","      return 0\n","\n","    return total_correct/total_inputs\n","\n","def label_func(fn): \n","    return str(fn).replace(\".png\", \"_label.png\").replace(\"train\", \"train_label\").replace(\"val/\", \"val_label/\")\n","\n","\n","# ----------------- Collect all the file names into two lists ------------------\n","\n","base_path = \"input/unet-lanes-v3/Dataset 3\"\n","additional_path = \"input/additional-data\"\n","\n","imagePaths = []\n","maskPaths = []\n","\n","validPaths = []\n","valid_lblPaths = []\n","\n","for folder in os.listdir(base_path):\n","    if os.path.isdir(base_path+\"/\"+folder):\n","        if folder == \"Modified Carla\":\n","            continue\n","        count = 0\n","        for subdir in os.listdir(base_path+\"/\"+folder):\n","            if subdir == \"inputs\":\n","                for filename in os.listdir(base_path+\"/\"+folder+\"/inputs\"):\n","                    if filename == \".DS_Store\":\n","                        continue\n","                        \n","                    if (folder == \"Augmented\"):\n","                        if (count < 2945):\n","                            imagePaths = imagePaths + [base_path+\"/\"+folder+\"/inputs/\"+filename]\n","                            mask_name = filename.split(\"Input\")[0] + \"Label\" + filename.split(\"Input\")[1]\n","                            maskPaths = maskPaths + [base_path+\"/\"+folder+\"/labels/\"+ mask_name]\n","                        else:\n","                            break\n","#                     elif (count > len(os.listdir(base_path+\"/\"+folder+\"/inputs\"))//2) and (folder == \"Modified Carla\"):\n","#                         validPaths = validPaths + [base_path+\"/\"+folder+\"/inputs/\"+filename]\n","#                         valid_lblPaths = valid_lblPaths + [base_path+\"/\"+folder+\"/labels/\"+filename.split(\".\")[0]+\"_Label.png\"]\n","                    else:    \n","                        imagePaths = imagePaths + [base_path+\"/\"+folder+\"/inputs/\"+filename]\n","                        maskPaths = maskPaths + [base_path+\"/\"+folder+\"/labels/\"+filename.split(\".\")[0]+\"_Label.png\"]\n","                    count += 1\n","                    \n","            elif folder == \"labels\":\n","                for filename in os.listdir(base_path+\"/\"+folder):\n","                    maskPaths = maskPaths + [base_path+\"/\"+folder+\"/\"+filename]\n","\n","# for folder in os.listdir(additional_path):\n","#     if os.path.isdir(additional_path+\"/\"+folder):\n","#         if folder == \"inputs\":\n","#             for filename in os.listdir(additional_path+\"/\"+folder):\n","#                 imagePaths = imagePaths + [additional_path+\"/\"+folder+\"/\"+filename]\n","#                 mask_name = filename.split(\".\")[0]+\"_Label.png\"\n","#                 maskPaths = maskPaths + [additional_path+\"/labels/\"+mask_name]\n","\n","# for i in range(3075):\n","#     if i > 2075:\n","#         imagePaths = imagePaths + [\"../input/lane-detection-for-carla-driving-simulator/train\"+\"/\"+os.listdir(\"../input/lane-detection-for-carla-driving-simulator/train\")[i]]\n","#         maskPaths = maskPaths + [label_func(\"../input/lane-detection-for-carla-driving-simulator/train\"+\"/\"+os.listdir(\"../input/lane-detection-for-carla-driving-simulator/train\")[i])]\n","#     else:\n","#         validPaths = validPaths + [\"../input/lane-detection-for-carla-driving-simulator/train\"+\"/\"+os.listdir(\"../input/lane-detection-for-carla-driving-simulator/train\")[i]]\n","#         valid_lblPaths = valid_lblPaths + [label_func(\"../input/lane-detection-for-carla-driving-simulator/train\"+\"/\"+os.listdir(\"../input/lane-detection-for-carla-driving-simulator/train\")[i])]  \n","\n","# for file in os.listdir(\"../input/lane-detection-for-carla-driving-simulator/val\"):\n","#     if file == \".DS_Store\":\n","#         continue\n","        \n","#     validPaths = validPaths + [\"../input/lane-detection-for-carla-driving-simulator/val/\"+file]\n","#     valid_lblPaths = valid_lblPaths + [\"../input/lane-detection-for-carla-driving-simulator/val_label/\"+file.split(\".\")[0]+\"_label.png\"]   \n","    \n","print(len(imagePaths),len(maskPaths))\n","# imagePaths = sorted(imagePaths)\n","# maskPaths = sorted(maskPaths)\n","\n","# ------------- Instantiate the custom dataset and dataloaders -----------------\n","# Do an 85% - 15% split of the images for training and validation\n","transform = transforms.Compose([transforms.ToPILImage(),\n","                                transforms.Resize((160,256)),\n","                                transforms.ToTensor()])\n","\n","all_idx = np.arange(0,len(imagePaths)).tolist()\n","all_valid_idx = np.arange(0,len(validPaths)).tolist()\n","random.shuffle(all_idx)\n","random.shuffle(all_valid_idx)\n","\n","split = int(np.ceil(0.2*len(all_idx)))\n","valid_idx_additional = all_idx[:split]\n","\n","train_images = []\n","train_labels = []\n","\n","valid_images = []\n","valid_labels = []\n","\n","for idx in all_idx[split:]:\n","    train_images.append(imagePaths[idx])\n","    train_labels.append(maskPaths[idx])\n","\n","for idx in all_valid_idx:\n","    valid_images.append(validPaths[idx])\n","    valid_labels.append(valid_lblPaths[idx])\n","\n","for idx in valid_idx_additional:\n","    valid_images.append(imagePaths[idx])\n","    valid_labels.append(maskPaths[idx])\n","    \n","print(f'{len(train_images)} train images,{len(train_labels)} train labels')\n","print(f'{len(valid_images)} val images,{len(valid_labels)} val labels')\n","\n","trainset = LaneDataset(train_images,train_labels,prob=0.15,transforms=transform)\n","validset = LaneDataset(valid_images,valid_labels,prob=0.6,transforms=transform)\n","\n","batch = 64\n","\n","trainloader = DataLoader(trainset,\n","                        batch_size=batch,\n","                        num_workers=0)\n","\n","validloader = DataLoader(validset,\n","                        batch_size=batch,\n","                        num_workers=0)\n","         "]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-10-15T04:41:42.673393Z","iopub.status.busy":"2022-10-15T04:41:42.673068Z","iopub.status.idle":"2022-10-15T04:41:44.570231Z","shell.execute_reply":"2022-10-15T04:41:44.568762Z","shell.execute_reply.started":"2022-10-15T04:41:42.673349Z"},"id":"Kq_KRUSZpjXQ","outputId":"81cb6c82-9f51-4209-9b2f-daffba0b003f","trusted":true},"outputs":[{"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Show test image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m i, (img_batch, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader):\n\u001b[0;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m (i \u001b[39m<\u001b[39m \u001b[39m2000\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}],"source":["# Show test image\n","for i, (img_batch, _) in enumerate(trainloader):\n","    if (i < 2000):\n","        continue\n","    img = img_batch[0]\n","    gradient = img[2]\n","    \n","    fig = plt.figure(figsize=(20, 30))\n","    \n","    fig.add_subplot(3, 1, 1)\n","    plt.imshow(gradient, \"gray\")\n","    fig.add_subplot(3, 1, 2)\n","\n","    plt.imshow(img[0], \"gray\")\n","    fig.add_subplot(3, 1, 3)\n","\n","    plt.imshow(img[1], \"gray\")\n","\n","    print(gradient.shape)\n","    break"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input/unet-lanes-v3/Dataset 3/Augmented/inputs/Augmented_Input_1034.png\n","input/unet-lanes-v3/Dataset 3/Augmented/labels/Augmented_Label_1034.png\n"]}],"source":["print(imagePaths[40])\n","print(maskPaths[40])"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.status.busy":"2022-10-15T04:41:44.571268Z","iopub.status.idle":"2022-10-15T04:41:44.571928Z","shell.execute_reply":"2022-10-15T04:41:44.571756Z","shell.execute_reply.started":"2022-10-15T04:41:44.571732Z"},"id":"Kq_KRUSZpjXQ","outputId":"81cb6c82-9f51-4209-9b2f-daffba0b003f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using CUDA.\n","Epoch: 1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 67/67 [04:25<00:00,  3.96s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Training Error: 0.9988887922205691 | Training Loss: 2.7208418205602847 | Number of non-binary: 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 17/17 [01:01<00:00,  3.60s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Error: 1.0 | Validation Loss: 2.6465096333447624 | Number of non-binary: 0\n","Epoch: 2\n"]},{"name":"stderr","output_type":"stream","text":[" 36%|███▌      | 24/67 [01:54<03:25,  4.77s/batch]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [39], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     42\u001b[0m \u001b[39mwith\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(trainloader, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mfor\u001b[39;00m i,data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tepoch,\u001b[39m0\u001b[39m):\n\u001b[0;32m     44\u001b[0m \u001b[39m#         print(\"Training Iteration: {}\".format(i+1))\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         images,labels, path \u001b[39m=\u001b[39m data\n\u001b[0;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\tqdm\\std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1177\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1180\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1181\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1182\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn [12], line 42\u001b[0m, in \u001b[0;36mLaneDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m             output_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(output_image)\n\u001b[0;32m     39\u001b[0m \u001b[39m#             output_image = self.transforms(image)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m#             edges = self.transforms(edges)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m#             edges_inv = self.transforms(edges_inv)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m             mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(mask)\n\u001b[0;32m     44\u001b[0m \u001b[39m#         output_image = torch.cat((output_image,edges),dim=0)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m#         output_image = torch.cat((output_image,edges_inv),dim=0)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         mask_binary \u001b[39m=\u001b[39m (mask\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:304\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    297\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:419\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    415\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    416\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         )\n\u001b[0;32m    418\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 419\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[0;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:265\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    262\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    264\u001b[0m     )\n\u001b[1;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], interpolation)\n","File \u001b[1;32mc:\\Users\\ammar\\Documents\\CodingProjects\\ART\\CV-Pipeline\\venv\\lib\\site-packages\\PIL\\Image.py:2008\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2000\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[0;32m   2001\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[0;32m   2002\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2003\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2004\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[0;32m   2005\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[0;32m   2006\u001b[0m         )\n\u001b[1;32m-> 2008\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# ---------------------- Initialize the training loop --------------------------\n","l_rate = 0.1\n","momentum = 0.9\n","num_epochs = 40   # Start smaller to actually make sure that the model is not overfitting due to data similarities\n","\n","train_loss = []\n","train_error = []\n","val_loss = []\n","val_error = []\n","epochs = []\n","lr_vals = []\n","min_loss = np.inf\n","\n","if torch.cuda.is_available():\n","    print('Using CUDA.')\n","    device = torch.device('cuda:0')\n","\n","model = UNet()\n","# model.load_state_dict(torch.load(\"./ensemble_model_batch64_scheduled_lr0.5_epochs45.pt\"))\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","    model = model.to(device)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.SGD(model.parameters(),lr=l_rate,momentum=momentum)\n","# optimizer = optim.Adam(unet.parameters(),lr=l_rate)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=5)\n","\n","for e in range(num_epochs):\n","    \n","    print(\"Epoch: {}\".format(e+1))\n","    \n","    total_train_loss = 0\n","    total_val_loss = 0\n","    total_train_error = 0\n","    total_val_error = 0\n","    num_train_iterations = 0\n","    num_val_iterations = 0\n","\n","    model.train()\n","    with tqdm.tqdm(trainloader, unit=\"batch\") as tepoch:\n","        for i,data in enumerate(tepoch,0):\n","    #         print(\"Training Iteration: {}\".format(i+1))\n","\n","            images,labels, path = data\n","            if torch.cuda.is_available():\n","                images = images.cuda()\n","                images = images.to(device)\n","                labels = labels.cuda()\n","                labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            out = model(images)\n","            pred = torch.sigmoid(out)\n","\n","            # Check if the mask is truly binary\n","            test_label = labels.detach().cpu().numpy()\n","            num_not_binary = np.where(((test_label>0)&(test_label<1)|(test_label>1)),1,0).sum()\n","\n","            # For calculating error\n","            pred_np = pred.detach()\n","            labels_np = labels.detach()\n","\n","    #         masked_pred = np.where(pred_np>0.5,1,0) # Threshold prediction\n","            masked_pred = (pred_np>0.5).int()\n","            correct = torch.sum(torch.bitwise_and(masked_pred,labels_np.type(torch.int32))).item()\n","            incorrect = torch.sum(torch.bitwise_xor(masked_pred,labels_np.type(torch.int32))).item()\n","            if correct + incorrect == 0:\n","                print(f'{correct} and {incorrect}')\n","                print(out.shape, masked_pred.shape)\n","    #         Debugging -----------------------------------------------\n","            if (labels_np==1).sum().item() == 0:\n","                print((labels_np>0).sum().item())\n","                print(path)\n","                plt.figure()\n","                plt.subplot(1,2,1)\n","                plt.imshow(images.detach().cpu().numpy().squeeze().transpose(1,2,0))\n","                plt.subplot(1,2,2)\n","                plt.imshow(labels_np.numpy().squeeze())\n","#             print(labels)\n","\n","            error = incorrect/(correct+incorrect)\n","            loss = criterion(out,labels) + dice_loss(masked_pred,labels)*math.exp(error)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_train_loss += loss.item()\n","            total_train_error += error\n","            num_train_iterations += 1\n","\n","    train_error.append(total_train_error/num_train_iterations)\n","    train_loss.append(total_train_loss/num_train_iterations)\n","#     exp_lr_scheduler.step()\n","\n","    print(f\"Training Error: {train_error[-1]} | Training Loss: {train_loss[-1]} | Number of non-binary: {num_not_binary}\")\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        \n","        with tqdm.tqdm(validloader, unit=\"batch\") as tepoch:\n","            for i,data in enumerate(tepoch,0):\n","    #             print(\"Validation Iteration: {}\".format(i+1))\n","\n","                images,labels, path = data\n","                if torch.cuda.is_available():\n","                    images = images.cuda()\n","                    labels = labels.cuda()\n","\n","                out = model(images)\n","                pred = torch.sigmoid(out)\n","\n","                # Check if the mask is truly binary\n","                test_label = labels.detach().cpu().numpy()\n","                num_not_binary = np.where(((test_label>0)&(test_label<1)|(test_label>1)),1,0).sum()\n","\n","                # For calculating error\n","                pred_np = pred.detach()\n","                labels_np = labels.detach()\n","\n","                masked_pred = (pred_np>0.5).int()\n","\n","                correct = torch.sum(torch.bitwise_and(masked_pred,labels_np.type(torch.int32))).item()\n","                incorrect = torch.sum(torch.bitwise_xor(masked_pred,labels_np.type(torch.int32))).item()\n","                error = incorrect/(correct+incorrect)\n","\n","                loss = criterion(out,labels) + dice_loss(masked_pred,labels)*math.exp(error)\n","\n","                total_val_loss += loss.item()\n","                total_val_error += error\n","                num_val_iterations += 1\n","                \n","        val_error.append(total_val_error/num_val_iterations)\n","        val_loss.append(total_val_loss/num_val_iterations)\n","    \n","    scheduler.step(total_val_loss/num_val_iterations)\n","    \n","    lr_vals.append(optimizer.param_groups[0]['lr'])\n","    \n","    print(f\"Validation Error: {val_error[-1]} | Validation Loss: {val_loss[-1]} | Number of non-binary: {num_not_binary}\")\n","\n","    if (val_loss[-1] < min_loss) and (e > 4):\n","        print(\"Saved epoch {}\".format(e+1))\n","        torch.save(model.state_dict(),f\"./unet_gray_model_batch{batch}_sheduled_lr{l_rate}_epochs{num_epochs}.pt\")\n","        min_loss = val_loss[-1]\n","        \n","    epochs.append(e+1)\n","\n","# f_train_accuracy = accuracy(unet,trainloader)\n","# # f_val_accuracy = accuracy(unet,validloader)\n","# # print(\"Final Training Error: {} | Final Validation Error: {}\".format(f_train_accuracy,f_val_accuracy))\n","# print(\"Final Training Error: {}\".format(f_train_accuracy))\n","\n","# ------------------ Plot the training and validation curves -------------------\n","fig = plt.figure()\n","ax1 = fig.add_subplot(1,3,1)\n","ax2 = fig.add_subplot(1,3,2)\n","ax3 = fig.add_subplot(1,3,3)\n","\n","ax1.plot(epochs,train_error,label=\"Training\")\n","ax1.plot(epochs,val_error,label=\"Validation\")\n","ax1.set_title(\"Model Error Curves\")\n","ax1.set_xlabel(\"Epoch\")\n","ax1.set_ylabel(\"Error\")\n","ax1.legend()\n","\n","ax2.plot(epochs,train_loss,label=\"Training\")\n","ax2.plot(epochs,val_loss,label=\"Validation\")\n","ax2.set_title(\"Model Loss Curves\")\n","ax2.set_xlabel(\"Epoch\")\n","ax2.set_ylabel(\"Loss\")\n","ax2.legend()\n","\n","ax3.plot(epochs,lr_vals,label=\"Learning Rate\")\n","ax3.set_title(\"Model LR\")\n","ax3.set_xlabel(\"Epoch\")\n","ax3.set_ylabel(\"Learning Rate\")\n","ax3.legend()\n","\n","fig.tight_layout()\n","plt.show()\n","\n","# torch.save(unet.state_dict(),\"Some Path\")\n","\n","# Loading model weights\n","# unet.load_state_dict(torch.load(\"Some Path\"))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.status.busy":"2022-10-15T04:41:44.573286Z","iopub.status.idle":"2022-10-15T04:41:44.573623Z","shell.execute_reply":"2022-10-15T04:41:44.573477Z","shell.execute_reply.started":"2022-10-15T04:41:44.573460Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'unet' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m----> 2\u001b[0m     unet\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      3\u001b[0m unet\u001b[39m.\u001b[39meval()\n\u001b[0;32m      4\u001b[0m \u001b[39m# unet.prob = 1\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'unet' is not defined"]}],"source":["if torch.cuda.is_available():\n","    unet.cuda()\n","unet.eval()\n","# unet.prob = 1\n","\n","orig = cv2.imread(\"../input/unet-lanes-v3/Dataset 3/Synthetic/inputs/Synthetic_Lane_100.png\")\n","test_im = np.copy(orig)\n","\n","median = np.median(cv2.cvtColor(test_im,cv2.COLOR_BGR2GRAY))\n","lower = int(max(0,(1-0.205)*median))\n","upper = int(min(255,(1+0.205)*median))\n","\n","test_edges = cv2.Canny(cv2.cvtColor(test_im,cv2.COLOR_BGR2GRAY),lower,upper)\n","test_edges_inv = cv2.bitwise_not(test_edges)\n","\n","# grad_x = cv2.Sobel(cv2.cvtColor(test_im,cv2.COLOR_BGR2GRAY), cv2.CV_16S, 1, 0, ksize=1, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)\n","# grad_y = cv2.Sobel(cv2.cvtColor(test_im,cv2.COLOR_BGR2GRAY), cv2.CV_16S, 0, 1, ksize=1, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)\n","\n","# abs_grad_x = cv2.convertScaleAbs(grad_x)\n","# abs_grad_y = cv2.convertScaleAbs(grad_y)\n","# grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n","\n","# test_im_edges = grad\n","\n","test_im = np.append(test_im,test_edges.reshape(test_edges.shape[0],test_edges.shape[1],1),axis=2)\n","test_im = np.append(test_im,test_edges_inv.reshape(test_edges_inv.shape[0],test_edges_inv.shape[1],1),axis=2)\n","\n","test_im = cv2.resize(test_im,(330,180))\n","ground_truth = cv2.imread(\"../input/unet-lanes-v3/Dataset 3/Synthetic/labels/Synthetic_Lane_100_Label.png\",cv2.IMREAD_GRAYSCALE)\n","ground_truth = cv2.resize(ground_truth,(330,180))\n","\n","# func = transforms.Compose([transforms.ToPILImage(),\n","#                                 transforms.Resize((180,330)),\n","#                                 transforms.ToTensor()])\n","# test_im = test_im/255\n","# test_img = func(test_im).unsqueeze(0).cuda()\n","test_img = (torch.Tensor((test_im/255.)).cuda().permute(2,0,1)).reshape(1,5,180,330)\n","\n","# print((test_img == test_img2).sum().item())\n","# print(test_img.shape, test_img2.shape)\n","# print(test_img)\n","# print(test_img2)\n","\n","output = unet(test_img)\n","pred = torch.sigmoid(output)\n","pred_np = pred.detach().cpu().numpy().squeeze()\n","\n","pred_mask = np.where(pred_np>0.5,1,0)\n","# print((pred_mask==ground_truth).sum())\n","# print((pred_mask!=ground_truth).sum())\n","# print(pred_mask==ground_truth)\n","print(pred_np)\n","# print((pred_np>0.005).sum())\n","pred_mask = pred_mask*255\n","plt.figure()\n","plt.subplot(1,3,1)\n","plt.imshow(pred_mask.astype('uint8'))\n","# plt.imshow(grad)\n","plt.subplot(1,3,2)\n","plt.imshow(ground_truth.astype('uint8'))\n","plt.subplot(1,3,3)\n","plt.imshow(cv2.cvtColor(orig,cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-10-15T04:41:44.575428Z","iopub.status.idle":"2022-10-15T04:41:44.576102Z","shell.execute_reply":"2022-10-15T04:41:44.575857Z","shell.execute_reply.started":"2022-10-15T04:41:44.575829Z"},"trusted":true},"outputs":[],"source":["# # Add this part into the custom dataset class and have a check that determines if \"carla\" or smth is in the \n","# # name in order to perform the necessary operations\n","\n","# input_pth = \"../input/unet-lanes-v3/Day Time/inputs/Lane_Input_2800.png\"\n","# label_pth = \"../input/unet-lanes-v3/Day Time/labels/Lane_Input_2800_Label.png\"\n","\n","# carla_im = cv2.imread(input_pth)\n","# carla_lbl = cv2.imread(label_pth)\n","# hls_im = cv2.cvtColor(carla_im,cv2.COLOR_BGR2HLS)\n","# print(carla_lbl.shape)\n","\n","# dashed_mask = np.zeros((carla_lbl.shape[0],carla_lbl.shape[1]))\n","# line_mask = np.zeros((carla_lbl.shape[0],carla_lbl.shape[1]))\n","# edges_mask = np.zeros((carla_im.shape[0],carla_im.shape[1]),dtype=np.int)\n","\n","# gray_im = cv2.cvtColor(carla_im,cv2.COLOR_BGR2GRAY)\n","# gray_lbl = cv2.cvtColor(carla_lbl,cv2.COLOR_BGR2GRAY)\n","\n","# # gray_im = cv2.GaussianBlur(gray_im,(5,5),0)\n","\n","# median = np.median(gray_im)\n","# lower = int(max(0,(1-0.205)*median))\n","# upper = int(min(255,(1+0.205)*median))\n","\n","# edges = cv2.bitwise_not(cv2.Canny(gray_im,lower,upper))\n","# h_lines = cv2.HoughLinesP(edges,1,np.pi/180,100,50,20)\n","\n","# # Quadrant division of input image\n","# width = carla_lbl.shape[1]\n","# height = carla_lbl.shape[0]\n","\n","# med1 = np.median(gray_im[:height//2,:width//2])\n","# med2 = np.median(gray_im[:height//2,width//2:])\n","# med3 = np.median(gray_im[height//2:,width//2:])\n","# med4 = np.median(gray_im[height//2:,:width//2])\n","\n","# l1 = int(max(0,(1-0.33)*med1))\n","# u1 = int(min(255,(1+0.33)*med1))\n","# e1 = cv2.Canny(gray_im[:height//2,:width//2],l1,u1)\n","\n","# l2 = int(max(0,(1-0.33)*med2))\n","# u2 = int(min(255,(1+0.33)*med2))\n","# e2 = cv2.Canny(gray_im[:height//2,width//2:],l2,u2)\n","\n","# l3 = int(max(0,(1-0.33)*med3))\n","# u3 = int(min(255,(1+0.33)*med3))\n","# e3 = cv2.Canny(gray_im[height//2:,width//2:],l3,u3)\n","\n","# l4 = int(max(0,(1-0.33)*med4))\n","# u4 = int(min(255,(1+0.33)*med4))\n","# e4 = cv2.Canny(gray_im[height//2:,:width//2],l4,u4)\n","\n","# # Stitch the edges together\n","# edges_mask[:height//2,:width//2] = e1\n","# edges_mask[:height//2,width//2:] = e2\n","# edges_mask[height//2:,width//2:] = e3\n","# edges_mask[height//2:,:width//2] = e4\n","\n","# edges_mask = cv2.bitwise_not(edges_mask)\n","\n","# print(len(h_lines))\n","\n","# if len(h_lines) != 0:\n","#     for l in range(0,len(h_lines)):\n","#         line = h_lines[l][0]\n","#         cv2.line(line_mask,(line[0],line[1]),(line[2],line[3]),1,3,cv2.LINE_AA)\n","#     binary_im = line_mask.astype(np.int64)\n","#     binary_lbl = np.where(gray_lbl>0,1,0)\n","# else:\n","#     mean_value = np.mean(gray_im[(gray_lbl>0).nonzero()])\n","#     max_value = np.max(gray_im[(gray_lbl>0).nonzero()])\n","#     min_value = np.min(gray_im[(gray_lbl>0).nonzero()])\n","\n","#     threshold = int(mean_value - 5)\n","\n","#     binary_lbl = np.where(gray_lbl>0,1,0)\n","#     binary_im = np.where(gray_im>=threshold,1,0)\n","\n","# dashed_mask = cv2.bitwise_and(binary_lbl,binary_im)\n","\n","# test = np.bitwise_or(np.bitwise_and(gray_lbl,edges_mask),np.bitwise_and(gray_lbl,np.bitwise_not(edges_mask)))\n","# print(type(test)==np.ndarray)\n","\n","# plt.figure()\n","# plt.subplot(1,3,1)\n","# plt.imshow(cv2.cvtColor(carla_im,cv2.COLOR_BGR2RGB))\n","# plt.subplot(1,3,2)\n","# # plt.imshow(255*cv2.cvtColor(carla_lbl,cv2.COLOR_BGR2GRAY))\n","# plt.imshow(np.bitwise_or(np.bitwise_and(gray_lbl,edges_mask),np.bitwise_and(gray_lbl,np.bitwise_not(edges_mask))))\n","# plt.subplot(1,3,3)\n","# plt.imshow(edges_mask)\n","\n","# set_im = iter(trainloader)\n","# imgs,lbls = set_im.next()\n","# # print(imgs[0])\n","# # print((lbls[0]==1).sum())"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.8 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"},"vscode":{"interpreter":{"hash":"803ee3eaa8a8fa442c9ec85c689725a507710d867752b4abd2cdc537bce80813"}}},"nbformat":4,"nbformat_minor":4}
